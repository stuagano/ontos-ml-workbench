---
name: Delta Table Schemas - Unity Catalog Foundation
status: completed
created: 2026-02-06T16:44:03Z
updated: 2026-02-06T17:30:00Z
completed: 2026-02-06T17:30:00Z
github: https://github.com/databricks-field-eng/mirion-workspace/issues/2
depends_on: []
parallel: false
conflicts_with: []
---

## Completion Notes

**Status:** ✅ COMPLETED (with modifications)

**Schema Location:** `home_stuart_gano.ontos_ml_workbench` (using home catalog instead of creating new catalog due to permissions)

**Tables Created:** 7 production tables (sheets, templates, canonical_labels, training_sheets, qa_pairs, model_training_lineage, example_store)

**Simplifications Made:**
- Removed DEFAULT values (enforced in app layer)
- Removed UNIQUE constraints (enforced in app layer)
- Removed CREATE INDEX (Delta uses automatic data skipping)
- Changed VARIANT → STRING for JSON columns

**Documentation:** See `schemas/COMPLETION_SUMMARY.md` for full details

**Next Task:** Ready for Task #3 (Sheet Management API)

# Task: Delta Table Schemas - Unity Catalog Foundation

## Description

Create all Delta Lake table schemas in Unity Catalog for the Ontos ML Workbench. This includes the core data model for Sheets, Templates, Canonical Labels, Training Sheets, Q&A Pairs, and Model Training Lineage. These tables form the foundation for the entire platform.

**Location:** `ontos_ml.workbench` catalog

## Acceptance Criteria

- [ ] `ontos_ml.workbench` catalog exists in Unity Catalog
- [ ] All 7 Delta tables created with proper schemas
- [ ] Composite key UNIQUE constraint on `canonical_labels(sheet_id, item_ref, label_type)`
- [ ] Foreign key relationships documented (not enforced - Delta doesn't support FKs)
- [ ] Table properties include descriptive comments
- [ ] DDL SQL scripts saved in `schemas/` directory
- [ ] Seed tables with 1-2 example rows for validation

## Technical Details

**Tables to Create:**

1. **sheets** - Dataset definitions (pointers to UC tables/volumes)
2. **templates** - Prompt templates with `label_type` field
3. **canonical_labels** - Ground truth with composite key `(sheet_id, item_ref, label_type)`
4. **training_sheets** - Q&A datasets
5. **qa_pairs** - Individual Q&A pairs with `canonical_label_id` linkage
6. **model_training_lineage** - Track which models used which Training Sheets
7. **example_store** - Few-shot examples (P1, create schema now for future)

**Key Schema Features:**
- Use VARIANT type for JSON columns (`label_data`, `label_schema`, etc.)
- Use ARRAY<STRING> for multi-value fields (`allowed_uses`, `prohibited_uses`)
- All tables have `created_at`, `created_by`, `updated_at`, `updated_by` audit fields
- Use STRING for IDs (UUIDs) not BIGINT

**Implementation Approach:**
- Create SQL DDL files in `schemas/` directory
- Execute via Databricks SQL or `databricks sql` CLI
- Use `CREATE TABLE IF NOT EXISTS` for idempotency
- Add table comments explaining purpose

**Files to Create:**
```
schemas/
├── 01_create_catalog.sql
├── 02_sheets.sql
├── 03_templates.sql
├── 04_canonical_labels.sql
├── 05_training_sheets.sql
├── 06_qa_pairs.sql
├── 07_model_training_lineage.sql
└── 08_example_store.sql
```

**Critical: Canonical Labels Composite Key**
```sql
CREATE TABLE canonical_labels (
  id STRING PRIMARY KEY,
  sheet_id STRING NOT NULL,
  item_ref STRING NOT NULL,
  label_type STRING NOT NULL,
  label_data VARIANT,
  ...
  CONSTRAINT unique_label UNIQUE (sheet_id, item_ref, label_type)
);
```

## Dependencies

- [ ] Databricks workspace with Unity Catalog enabled
- [ ] `ontos_ml` catalog exists (or create it)
- [ ] User has CREATE TABLE permissions in Unity Catalog

## Effort Estimate

- Size: M
- Hours: 8-12 hours
- Parallel: false (blocks all other tasks)

## Definition of Done

- [ ] All SQL DDL files created in `schemas/`
- [ ] Tables created in Unity Catalog `ontos_ml.workbench`
- [ ] Verified via `SHOW TABLES IN ontos_ml.workbench`
- [ ] Composite key constraint on canonical_labels working (test insert duplicate)
- [ ] README.md in `schemas/` directory documenting table relationships
- [ ] Backend can successfully query all tables via Databricks SDK
