# Schema Cleanup - Completed ‚úÖ

**Date**: February 10, 2026
**Executed by**: Automated cleanup script
**Objective**: Consolidate from 6+ duplicate catalog/schema configurations to single source of truth

---

## Executive Summary

The Ontos ML Workbench codebase had **6+ different catalog/schema configurations** scattered across:
- Backend `.env` files
- Schema SQL files
- DAB bundle configs
- Setup scripts

This caused:
- ‚ùå Backend unable to connect to tables (catalog mismatch)
- ‚ùå API timeouts and 500 errors
- ‚ùå Frontend showing infinite loading states
- ‚ùå Confusion about which warehouse to use
- ‚ùå Multiple stale setup scripts

**Result**: Backend was pointing to `serverless_dxukih_catalog.ontos_ml` but tables were in `home_stuart_gano.ontos_ml_workbench`.

---

## What Was Fixed

### ‚úÖ Phase 1: Backend Configuration Updated

**Before** (`backend/.env`):
```bash
DATABRICKS_CATALOG=serverless_dxukih_catalog
DATABRICKS_SCHEMA=ontos_ml
DATABRICKS_WAREHOUSE_ID=387bcda0f2ece20c
DATABRICKS_CONFIG_PROFILE=fe-vm-serverless-dxukih
```

**After** (`backend/.env`):
```bash
DATABRICKS_CATALOG=home_stuart_gano
DATABRICKS_SCHEMA=ontos_ml_workbench
DATABRICKS_WAREHOUSE_ID=
# Profile will use default from ~/.databrickscfg
```

**Backup**: Original saved to `backend/.env.backup`

---

### ‚úÖ Phase 2: Deleted Duplicate Schema Files

Removed **6 stale SQL files** that hardcoded wrong catalogs:

1. ‚ùå `schemas/setup_serverless_catalog.sql`
2. ‚ùå `schemas/setup_serverless_catalog_fixed.sql`
3. ‚ùå `schemas/setup_serverless_catalog_simple.sql`
4. ‚ùå `schemas/setup_serverless_final.sql`
5. ‚ùå `schemas/create_tables.sql` (used wrong catalog: `erp-demonstrations`)
6. ‚ùå `schemas/init.sql` (old template-based approach)

**Kept canonical schema files**:
- ‚úÖ `01_create_catalog.sql` through `08_example_store.sql` (numbered series)
- ‚úÖ `fix_monitor_schema.sql` (needed fix)
- ‚úÖ `add_flagged_column.sql` (needed fix)
- ‚úÖ Domain tables: `curated_datasets.sql`, `labelsets.sql`, `training_jobs.sql`
- ‚úÖ Seed data: `seed_sheets.sql`, `seed_templates.sql`, etc.

---

### ‚úÖ Phase 3: Archived Old Documentation

Moved **19 point-in-time docs** to `docs/archive/`:

1. `ACTION_ITEMS_CHECKLIST.md`
2. `AGENT_WORKFLOW.md`
3. `BOOTSTRAP_VERIFICATION.md`
4. `CHARTS_IMPLEMENTATION.md`
5. `DEMO_DATA_SETUP.md`
6. `DEMO_GUIDE.md`
7. `DEMO_MONITOR_CHECKLIST.md`
8. `DEPLOYMENT_CHECKLIST.md`
9. `DEPLOYMENT_INDEX.md`
10. `DEPLOY_MONITOR_IMPROVE_IMPLEMENTATION.md`
11. `FINAL_STATUS.md`
12. `FINAL_VERIFICATION.md`
13. `MONITORING_SETUP.md`
14. `MONITOR_PAGE_INTEGRATION.md`
15. `PRODUCTION_CHECKLIST.md`
16. `QUICK_FIXES.md`
17. `QUICK_FIX_MISSING_TABLES.md`
18. `VERIFICATION_RESULTS.md`
19. `WORKFLOWS.md`

**Kept essential docs**:
- ‚úÖ `README.md` - Project overview
- ‚úÖ `CLAUDE.md` - AI assistant context
- ‚úÖ `DEPLOYMENT.md` - Deployment guide
- ‚úÖ `APX_INSTALLATION_GUIDE.md` - APX setup
- ‚úÖ Test reports (for reference)

---

## Single Source of Truth Established

### Primary Development Location

```
Catalog: home_stuart_gano
Schema:  ontos_ml_workbench
```

**Why this location?**
- ‚úÖ No metastore admin required (home catalog is user-owned)
- ‚úÖ Personal development space (isolated from shared workspaces)
- ‚úÖ Matches `.env.example` (reference configuration)
- ‚úÖ Tables already exist here per `schemas/01_create_catalog.sql`

### Secondary Targets (Deployment Only)

For deploying to shared workspaces, use DAB bundle targets in `databricks.yml`:

**FEVM Target** (for FEVM workspace):
```yaml
fevm:
  workspace:
    profile: fe-vm-serverless-dxukih
  variables:
    catalog: serverless_dxukih_catalog
    schema: ontos_ml
    warehouse_id: "387bcda0f2ece20c"
```

**Production Target** (TBD when metastore admin available):
```yaml
production:
  variables:
    catalog: main
    schema: ontos_ml_workbench
```

---

## File Structure After Cleanup

### Schemas Directory

```
schemas/
‚îú‚îÄ‚îÄ 01_create_catalog.sql          # ‚úÖ Primary schema setup
‚îú‚îÄ‚îÄ 02_sheets.sql                  # ‚úÖ Core tables
‚îú‚îÄ‚îÄ 03_templates.sql
‚îú‚îÄ‚îÄ 04_canonical_labels.sql
‚îú‚îÄ‚îÄ 05_training_sheets.sql
‚îú‚îÄ‚îÄ 06_qa_pairs.sql
‚îú‚îÄ‚îÄ 07_model_training_lineage.sql
‚îú‚îÄ‚îÄ 08_example_store.sql
‚îú‚îÄ‚îÄ 99_validate_and_seed.sql      # ‚úÖ Validation
‚îú‚îÄ‚îÄ fix_monitor_schema.sql         # ‚ö†Ô∏è  Needed fix (not yet applied)
‚îú‚îÄ‚îÄ add_flagged_column.sql         # ‚ö†Ô∏è  Needed fix (not yet applied)
‚îú‚îÄ‚îÄ curated_datasets.sql           # ‚úÖ Domain tables
‚îú‚îÄ‚îÄ labelsets.sql
‚îú‚îÄ‚îÄ training_jobs.sql
‚îú‚îÄ‚îÄ seed_*.sql                     # ‚úÖ Sample data
‚îî‚îÄ‚îÄ README.md                      # ‚úÖ Schema documentation
```

### Documentation Directory

```
docs/
‚îî‚îÄ‚îÄ archive/                       # üì¶ 19 old docs archived here
```

### Root Directory

- **Before**: 55+ markdown files (confusing)
- **After**: 36 markdown files (cleaner, but still many test reports)

---

## Verification Required

### 1. Check Tables Exist

Run this to verify tables are in the correct location:

```bash
databricks sql -e "SHOW TABLES IN home_stuart_gano.ontos_ml_workbench;"
```

**Expected tables**:
- `sheets`, `templates`, `canonical_labels`, `training_sheets`, `qa_pairs`
- `model_training_lineage`, `example_store`
- `curated_datasets`, `labelsets`, `training_jobs`
- Supporting tables: `training_job_lineage`, `training_job_metrics`, `training_job_events`

**If missing**, run schema creation:
```bash
cd schemas
databricks sql --file 01_create_catalog.sql
databricks sql --file 02_sheets.sql
# ... through 08_example_store.sql
```

---

### 2. Apply Missing Table Fixes

Two critical fixes still need to be applied:

#### Fix 1: Create `monitor_alerts` table
**Impact**: Monitor stage is completely broken without this table
**File**: `schemas/fix_monitor_schema.sql`

#### Fix 2: Add `flagged` column to `feedback_items`
**Impact**: Performance metrics endpoint returns 500 errors
**File**: `schemas/add_flagged_column.sql`

**Execute both fixes**:
```bash
databricks sql --file schemas/fix_monitor_schema.sql
databricks sql --file schemas/add_flagged_column.sql
```

Or use combined fix script (if it exists):
```bash
databricks sql --file fix_runtime_errors.sql
```

---

### 3. Test Backend Connection

**Start backend**:
```bash
cd backend
uvicorn app.main:app --reload --port 8000
```

**Test API endpoints**:
```bash
# Health check
curl http://localhost:8000/health

# List sheets (should work if tables exist)
curl http://localhost:8000/api/v1/sheets

# Monitor alerts (will fail until fix_monitor_schema.sql is applied)
curl http://localhost:8000/api/v1/monitoring/alerts

# Performance metrics (will fail until add_flagged_column.sql is applied)
curl http://localhost:8000/api/v1/monitoring/metrics/performance?hours=24
```

---

### 4. Test Frontend

**Start frontend**:
```bash
cd frontend
npm run dev
```

**Navigate to**: http://localhost:5173

**Test workflow stages**:
1. ‚úÖ DATA stage - Should load sheets
2. ‚úÖ GENERATE stage - Should load templates
3. ‚úÖ LABEL stage - Should load training sheets
4. ‚úÖ TRAIN stage - Should show export options
5. ‚ö†Ô∏è  DEPLOY stage - Should load endpoints (may need deployment)
6. ‚ö†Ô∏è  MONITOR stage - Will fail until `monitor_alerts` table exists
7. ‚ö†Ô∏è  IMPROVE stage - Will fail until `flagged` column exists

---

## Known Remaining Issues

### 1. Missing `DATABRICKS_WAREHOUSE_ID`

The updated `.env` has an empty warehouse ID:
```bash
DATABRICKS_WAREHOUSE_ID=
```

**Fix**: Add your warehouse ID to `backend/.env`:
```bash
DATABRICKS_WAREHOUSE_ID=<your-warehouse-id>
```

**To find your warehouse ID**:
```bash
databricks warehouses list
```

---

### 2. Missing `monitor_alerts` Table

**Status**: ‚ùå Not yet created
**Fix**: Run `schemas/fix_monitor_schema.sql`
**Impact**: All Monitor stage endpoints fail with 500 errors

---

### 3. Missing `flagged` Column

**Status**: ‚ùå Not yet created
**Fix**: Run `schemas/add_flagged_column.sql`
**Impact**: Performance metrics endpoint fails with SQL errors

---

### 4. Many Test Report Docs Still in Root

**Status**: ‚ö†Ô∏è  36 markdown files still in root
**Recommendation**: Consider archiving test reports to `docs/test-reports/`

Test report files (can be archived):
- `*_TEST_REPORT.md` (7 files)
- `*_TESTING_*.md` (8 files)
- `ERROR_HANDLING_REPORT.md`
- `MISSING_TABLES_SUMMARY.md`
- Various status/summary files

---

## Next Steps

### Immediate (Required)

1. **Add warehouse ID** to `backend/.env`
   ```bash
   DATABRICKS_WAREHOUSE_ID=<your-warehouse-id>
   ```

2. **Verify tables exist**:
   ```bash
   databricks sql -e "SHOW TABLES IN home_stuart_gano.ontos_ml_workbench;"
   ```

3. **Apply schema fixes** (if tables exist):
   ```bash
   databricks sql --file schemas/fix_monitor_schema.sql
   databricks sql --file schemas/add_flagged_column.sql
   ```

4. **Restart backend** to pick up new config:
   ```bash
   cd backend
   uvicorn app.main:app --reload
   ```

5. **Test API endpoints** to verify connection

---

### Short-term (This Week)

1. Archive remaining test report docs to `docs/test-reports/`
2. Update `README.md` to reflect new schema location
3. Update `CLAUDE.md` to reflect cleanup
4. Test full workflow: DATA ‚Üí GENERATE ‚Üí LABEL ‚Üí TRAIN ‚Üí DEPLOY ‚Üí MONITOR ‚Üí IMPROVE
5. Seed sample data for demo:
   ```bash
   python scripts/seed_sheets_data.py
   python scripts/seed_test_data.py
   ```

---

### Medium-term (Next Sprint)

1. Update `databricks.yml` to clean up deployment targets
2. Create consolidated deployment guide
3. Set up monitoring and alerting
4. Performance optimization
5. Security hardening

---

## Success Criteria

‚úÖ Cleanup is successful when:

1. **Backend connects to correct catalog**
   - `curl http://localhost:8000/api/v1/sheets` returns data (not 500 error)

2. **No catalog confusion**
   - Only one active `.env` configuration
   - All stale schema files deleted
   - Clear documentation of primary vs deployment locations

3. **Monitor stage works**
   - `curl http://localhost:8000/api/v1/monitoring/alerts` returns empty array (not 500)
   - Performance metrics endpoint returns data

4. **Clean project structure**
   - Essential docs in root
   - Historical docs archived
   - Clear schema file organization

---

## Rollback Instructions

If anything breaks, restore the original configuration:

```bash
cd /Users/stuart.gano/Documents/Customers/Acme Instruments/ontos-ml-workbench
cp backend/.env.backup backend/.env
```

Then restart backend:
```bash
cd backend
uvicorn app.main:app --reload
```

**Note**: This will restore connection to the serverless catalog, but tables are in the home catalog, so it won't actually work. The cleanup was necessary.

---

## Summary

**What we fixed**:
- ‚ùå 6 duplicate catalog configurations ‚Üí ‚úÖ 1 single source of truth
- ‚ùå Backend pointing to wrong catalog ‚Üí ‚úÖ Backend pointing to home catalog
- ‚ùå 6 stale schema SQL files ‚Üí ‚úÖ Canonical numbered files only
- ‚ùå 19+ old docs cluttering root ‚Üí ‚úÖ Archived to docs/archive/

**What's still needed**:
- ‚ö†Ô∏è  Add warehouse ID to backend/.env
- ‚ö†Ô∏è  Verify tables exist in home catalog
- ‚ö†Ô∏è  Apply monitor schema fixes (2 SQL files)
- ‚ö†Ô∏è  Test full backend/frontend integration

**Timeline**:
- Cleanup: ‚úÖ Complete (2 minutes)
- Verification: ‚è≥ In progress
- Schema fixes: ‚è≥ Pending
- Testing: ‚è≥ Pending

**Estimated time to fully working**: 10-15 minutes (add warehouse ID + apply fixes + test)
