"""Pydantic models for Training Sheets (AssembledDataset) - materialized Q&A pairs.

PRD v2.3: Training Sheets are materialized Q&A pairs generated from Sheets + Templates.
- Supports canonical label lookup for automatic pre-approval
- Tracks canonical label reuse statistics
- Includes usage constraints for data governance

Following GCP Vertex AI pattern:
- assembly_table, assembly = dataset.assemble(template_config)
- The assembly is the concrete result of applying the template to the dataset
- Contains actual rendered prompts and responses ready for labeling/training
"""

from datetime import datetime
from enum import Enum
from typing import Any

from pydantic import BaseModel, Field

from app.models.sheet import TemplateConfig


class AssemblyStatus(str, Enum):
    """Assembly lifecycle status."""

    ASSEMBLING = "assembling"  # Currently being assembled
    READY = "ready"  # Assembly complete, ready for use
    FAILED = "failed"  # Assembly failed
    ARCHIVED = "archived"


class ResponseSource(str, Enum):
    """Source of the response value (PRD v2.3: added canonical)."""

    EMPTY = "empty"  # No response yet
    AI_GENERATED = "ai_generated"  # Generated by AI
    HUMAN_LABELED = "human_labeled"  # Labeled by human
    HUMAN_VERIFIED = "human_verified"  # AI-generated but verified/corrected by human
    CANONICAL = "canonical"  # PRD v2.3: Pre-approved via canonical label lookup


class LabelingMode(str, Enum):
    """How the Q&A pair was labeled (PRD v2.3)."""

    AI_GENERATED = "ai_generated"  # AI-generated response
    MANUAL = "manual"  # Human-written response
    EXISTING_COLUMN = "existing_column"  # Mapped from existing data column
    CANONICAL = "canonical"  # PRD v2.3: Reused from canonical label


class AssembledRow(BaseModel):
    """A single assembled row with rendered prompt and response (PRD v2.3: added canonical label support)."""

    row_index: int = Field(..., description="Index into the source sheet")

    # The actual rendered prompt (template + data merged)
    prompt: str = Field(..., description="Fully rendered prompt with data substituted")

    # Source data snapshot (for reference/debugging)
    source_data: dict[str, Any] = Field(
        default_factory=dict,
        description="Snapshot of source column values used to render the prompt",
    )

    # PRD v2.3: Reference to source item for canonical label lookup
    item_ref: str | None = Field(
        default=None,
        description="Reference to source item (e.g., inspection_id, invoice_id) for canonical label lookup",
    )

    # Response
    response: str | None = Field(
        default=None,
        description="The response value (AI-generated or human-labeled)",
    )
    response_source: ResponseSource = Field(
        default=ResponseSource.EMPTY,
        description="How the response was produced",
    )

    # PRD v2.3: Canonical label linkage
    canonical_label_id: str | None = Field(
        default=None,
        description="Link to canonical label (if response from canonical label)",
    )
    labeling_mode: LabelingMode = Field(
        default=LabelingMode.AI_GENERATED, description="How this Q&A pair was labeled"
    )

    # Metadata
    generated_at: datetime | None = None
    labeled_at: datetime | None = None
    labeled_by: str | None = None
    verified_at: datetime | None = None
    verified_by: str | None = None

    # Quality flags
    is_flagged: bool = Field(default=False, description="Flagged for review")
    flag_reason: str | None = None
    confidence_score: float | None = Field(
        default=None, ge=0, le=1, description="AI confidence in the response"
    )
    notes: str | None = Field(default=None, description="Additional notes from labeler")

    # PRD v2.3: Usage constraints (from canonical label or set directly)
    allowed_uses: list[str] | None = Field(
        default=None,
        description="Permitted usage types (training, validation, evaluation, few_shot, testing)",
    )
    prohibited_uses: list[str] | None = Field(
        default=None, description="Explicitly forbidden usage types"
    )


class AssembledDataset(BaseModel):
    """
    Materialized result of applying a template to a sheet.

    This is the concrete output of sheet.assemble() - actual prompt/response pairs
    that can be used for labeling, review, and fine-tuning export.
    """

    id: str

    # Source references
    sheet_id: str = Field(..., description="ID of the source sheet")
    sheet_name: str | None = Field(
        default=None, description="Name of source sheet at assembly time"
    )

    # Frozen template config (snapshot at assembly time)
    template_config: TemplateConfig = Field(
        ..., description="Snapshot of template config used for this assembly"
    )

    # PRD v2.3: Label type for canonical label matching
    template_label_type: str | None = Field(
        default=None,
        description="Label type from template (for canonical label lookup)",
    )

    # Assembly metadata
    status: AssemblyStatus = AssemblyStatus.ASSEMBLING
    total_rows: int = Field(default=0, description="Total number of assembled rows")

    # Statistics (PRD v2.3: added canonical_reused_count)
    ai_generated_count: int = Field(
        default=0, description="Rows with AI-generated responses"
    )
    human_labeled_count: int = Field(default=0, description="Rows with human labels")
    human_verified_count: int = Field(default=0, description="Rows verified by humans")
    canonical_reused_count: int = Field(
        default=0, description="PRD v2.3: Rows pre-approved via canonical labels"
    )
    flagged_count: int = Field(default=0, description="Rows flagged for review")
    empty_count: int = Field(default=0, description="Rows without responses")

    # Timestamps
    created_at: datetime | None = None
    created_by: str | None = None
    updated_at: datetime | None = None
    completed_at: datetime | None = None  # When assembly finished

    # Error info (if status == FAILED)
    error_message: str | None = None

    class Config:
        from_attributes = True


# ============================================================================
# Request/Response Models
# ============================================================================


class AssembleRequest(BaseModel):
    """Request body for assembling a sheet."""

    # Optional: override the sheet's attached template
    template_config: TemplateConfig | None = Field(
        default=None,
        description="Optional template override. If not provided, uses sheet's attached template.",
    )

    # Options
    row_limit: int | None = Field(
        default=None,
        ge=1,
        le=100000,
        description="Limit number of rows to assemble (for preview/testing)",
    )
    generate_responses: bool = Field(
        default=False,
        description="If True, also run AI generation on assembled rows",
    )


class AssembleResponse(BaseModel):
    """Response from assembly operation."""

    assembly_id: str
    sheet_id: str
    status: AssemblyStatus
    total_rows: int
    message: str | None = None


class AssemblyPreviewResponse(BaseModel):
    """Response for previewing assembled rows."""

    assembly_id: str
    rows: list[AssembledRow]
    total_rows: int
    preview_rows: int

    # Pagination info
    offset: int = Field(default=0, description="Current offset in result set")
    limit: int = Field(default=100, description="Limit used for this query")

    # Stats
    ai_generated_count: int
    human_labeled_count: int
    human_verified_count: int
    flagged_count: int
    empty_count: int = Field(default=0, description="Rows without responses")


class AssemblyRowUpdate(BaseModel):
    """Request to update a single assembled row."""

    response: str | None = Field(default=None, description="New response value")
    is_flagged: bool | None = Field(default=None, description="Flag for review")
    flag_reason: str | None = Field(default=None, description="Reason for flagging")
    mark_as_verified: bool = Field(default=False, description="Mark as human verified")


class AssemblyGenerateRequest(BaseModel):
    """Request to generate AI responses for assembled rows."""

    row_indices: list[int] | None = Field(
        default=None,
        description="Specific rows to generate. If None, generates all empty rows.",
    )
    overwrite_existing: bool = Field(
        default=False,
        description="If True, regenerate even if response exists",
    )
    include_examples: bool = Field(
        default=True,
        description="Include human-labeled rows as few-shot examples",
    )


class AssemblyGenerateResponse(BaseModel):
    """Response from AI generation on assembly."""

    assembly_id: str
    generated_count: int
    failed_count: int
    errors: list[dict[str, Any]] | None = None


class AssemblyExportRequest(BaseModel):
    """Request to export assembly for fine-tuning."""

    # Export destination
    volume_path: str = Field(
        ...,
        description="UC Volume path for JSONL output (e.g., /Volumes/catalog/schema/vol/data.jsonl)",
    )

    # Filter options
    include_sources: list[ResponseSource] | None = Field(
        default=None,
        description="Only include rows with these response sources. Default: human_labeled, human_verified",
    )
    exclude_flagged: bool = Field(
        default=True,
        description="Exclude flagged rows from export",
    )

    # Format options
    include_system_instruction: bool = Field(
        default=True,
        description="Include system instruction in training examples",
    )
    format: str = Field(
        default="openai_chat",
        description="Export format: openai_chat, anthropic, or gemini",
    )

    # Train/validation split
    train_split: float | None = Field(
        default=None,
        ge=0.5,
        le=0.95,
        description="Fraction for training set (0.5-0.95). If provided, creates separate train/val files.",
    )
    random_seed: int = Field(
        default=42,
        description="Random seed for reproducible train/val splits",
    )


class AssemblyExportResponse(BaseModel):
    """Response from export operation."""

    assembly_id: str
    volume_path: str
    examples_exported: int
    format: str
    excluded_count: int = Field(default=0, description="Rows excluded due to filters")

    # Train/val split info (when train_split is provided)
    train_path: str | None = Field(default=None, description="Path to training JSONL")
    val_path: str | None = Field(default=None, description="Path to validation JSONL")
    train_count: int | None = Field(
        default=None, description="Number of training examples"
    )
    val_count: int | None = Field(
        default=None, description="Number of validation examples"
    )


class AssemblyListResponse(BaseModel):
    """Response for listing assemblies."""

    assemblies: list[AssembledDataset]
    total: int
    page: int
    page_size: int
