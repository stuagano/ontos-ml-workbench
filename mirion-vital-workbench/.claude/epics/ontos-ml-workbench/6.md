---
name: Q&A Generation Engine - Training Sheet Creation
status: open
created: 2026-02-06T16:44:03Z
updated: 2026-02-06T16:44:03Z
github: https://github.com/databricks-field-eng/mirion-workspace/issues/6
depends_on: [2, 3, 4, 5]
parallel: false
conflicts_with: []
---

# Task: Q&A Generation Engine - Training Sheet Creation

## Description

Build the core generation engine that creates Training Sheets by applying a Prompt Template to a Sheet. This is the heart of the system. The engine must:
1. Query source data from Sheet's Unity Catalog tables/volumes
2. Check for existing Canonical Labels (label_type match)
3. If canonical label exists → create pre-approved Q&A pair
4. If no canonical label → generate via Foundation Model API (status: unlabeled)
5. Create Training Sheet and Q&A pairs in Delta tables

**Key Innovation:** Automatic canonical label lookup for "label once, reuse everywhere".

## Acceptance Criteria

- [ ] API endpoint: POST `/api/v1/training-sheets/generate`
- [ ] Accepts `sheet_id`, `template_id`, and generation config
- [ ] Queries Sheet to get source data (Unity Catalog tables/volumes)
- [ ] For each item: looks up canonical label by `(sheet_id, item_ref, template.label_type)`
- [ ] If canonical label found: creates Q&A pair with `status=labeled`, `canonical_label_id` set
- [ ] If no canonical label: calls Foundation Model API, creates Q&A pair with `status=unlabeled`
- [ ] Creates Training Sheet record with generation metadata
- [ ] Inserts all Q&A pairs into `qa_pairs` table
- [ ] Returns Training Sheet ID and generation statistics
- [ ] Async generation for large Sheets (Databricks Job)

## Technical Details

**Generation Algorithm:**
```python
# Pseudocode for generation logic
for item in sheet.get_items():
    # 1. Check for canonical label
    canonical_label = canonical_label_service.get_by_composite_key(
        sheet_id=sheet.id,
        item_ref=item.ref,
        label_type=template.label_type
    )

    # 2. Render prompt
    user_prompt = template_service.render_template(
        template.user_prompt_template,
        data=item.data
    )

    # 3. Create Q&A pair
    if canonical_label:
        # Pre-approved! Expert already labeled this
        qa_pair = QAPair(
            messages=[
                {"role": "system", "content": template.system_prompt},
                {"role": "user", "content": user_prompt},
                {"role": "assistant", "content": canonical_label.label_data}
            ],
            status="labeled",  # Pre-approved
            labeling_mode="canonical",
            canonical_label_id=canonical_label.id,
            reviewed_by=canonical_label.labeled_by,
            reviewed_at=canonical_label.labeled_at,
            allowed_uses=canonical_label.allowed_uses,
            prohibited_uses=canonical_label.prohibited_uses
        )
    else:
        # No canonical label - generate via LLM
        ai_response = foundation_model_api.generate(
            system=template.system_prompt,
            user=user_prompt,
            model=template.base_model,
            temperature=template.temperature,
            max_tokens=template.max_tokens
        )
        qa_pair = QAPair(
            messages=[
                {"role": "system", "content": template.system_prompt},
                {"role": "user", "content": user_prompt},
                {"role": "assistant", "content": ai_response}
            ],
            status="unlabeled",  # Needs expert review
            labeling_mode="ai_generated",
            canonical_label_id=None
        )

    qa_pairs.append(qa_pair)

# 4. Create Training Sheet
training_sheet = TrainingSheet(
    sheet_id=sheet.id,
    template_id=template.id,
    total_pairs=len(qa_pairs),
    labeled_pairs=sum(1 for qa in qa_pairs if qa.status == "labeled"),
    unlabeled_pairs=sum(1 for qa in qa_pairs if qa.status == "unlabeled"),
    status="review" if unlabeled_count > 0 else "approved"
)
```

**Pydantic Models:**
```python
# backend/app/models/training_sheet.py
class TrainingSheet(BaseModel):
    id: str
    sheet_id: str
    template_id: str
    status: Literal["generating", "review", "approved", "trained"]
    total_pairs: int
    labeled_pairs: int = 0
    unlabeled_pairs: int = 0
    rejected_pairs: int = 0
    flagged_pairs: int = 0
    generation_config: Dict
    created_at: datetime
    created_by: str

class QAPair(BaseModel):
    id: str
    training_sheet_id: str
    messages: List[Dict]  # [{"role": "...", "content": "..."}]
    status: Literal["unlabeled", "labeled", "rejected", "flagged"]
    labeling_mode: Literal["canonical", "ai_generated", "manual", "existing_column"]
    canonical_label_id: Optional[str] = None
    allowed_uses: List[str] = []
    prohibited_uses: List[str] = []
    created_at: datetime
```

**API Endpoint:**
```python
@router.post("/training-sheets/generate")
async def generate_training_sheet(
    request: GenerateRequest,
    background_tasks: BackgroundTasks,
    db: DatabricksSession
):
    # For small Sheets: synchronous
    # For large Sheets: launch Databricks Job
    pass
```

**Files to Create/Modify:**
```
backend/app/
├── models/training_sheet.py
├── models/qa_pair.py
├── api/v1/training_sheets.py
├── services/generation_service.py
├── services/foundation_model_service.py
└── jobs/generate_training_sheet.py (Databricks Job)
```

## Dependencies

- [ ] Task 001 (Delta tables)
- [ ] Task 002 (Sheet API)
- [ ] Task 003 (Template API)
- [ ] Task 004 (Canonical Label API)
- [ ] Foundation Model API credentials configured
- [ ] Databricks SDK for Foundation Model APIs

## Effort Estimate

- Size: XL
- Hours: 24-32 hours
- Parallel: false (depends on Tasks 002-004)

## Definition of Done

- [ ] Generation endpoint creates Training Sheet + Q&A pairs
- [ ] Canonical label lookup works (pre-approved pairs marked as `labeled`)
- [ ] AI-generated pairs marked as `unlabeled` for review
- [ ] Foundation Model API integration working (tested with 10+ items)
- [ ] Generation statistics accurate (labeled vs unlabeled counts)
- [ ] Large Sheets (>100 items) use async Databricks Job
- [ ] Manual test: Generate Training Sheet for PCB defect dataset
- [ ] Manual test: Generate with existing canonical labels → auto-approved
- [ ] Backend CLAUDE.md updated with generation patterns
