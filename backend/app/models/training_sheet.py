"""Pydantic models for Training Sheets - materialized Q&A pairs.

PRD v2.3: Training Sheets are materialized Q&A pairs generated from Sheets + Templates.
- Supports canonical label lookup for automatic pre-approval
- Tracks canonical label reuse statistics
- Includes usage constraints for data governance
"""

from datetime import datetime
from enum import Enum
from typing import Any

from pydantic import BaseModel, Field

from app.models.sheet import TemplateConfig


class TrainingSheetStatus(str, Enum):
    """Training Sheet lifecycle status."""

    ASSEMBLING = "assembling"  # Currently being generated
    READY = "ready"  # Generation complete, ready for use
    FAILED = "failed"  # Generation failed
    ARCHIVED = "archived"


class ResponseSource(str, Enum):
    """Source of the response value (PRD v2.3: added canonical)."""

    EMPTY = "empty"  # No response yet
    AI_GENERATED = "ai_generated"  # Generated by AI
    HUMAN_LABELED = "human_labeled"  # Labeled by human
    HUMAN_VERIFIED = "human_verified"  # AI-generated but verified/corrected by human
    CANONICAL = "canonical"  # PRD v2.3: Pre-approved via canonical label lookup


class LabelingMode(str, Enum):
    """How the Q&A pair was labeled (PRD v2.3)."""

    AI_GENERATED = "ai_generated"  # AI-generated response
    MANUAL = "manual"  # Human-written response
    EXISTING_COLUMN = "existing_column"  # Mapped from existing data column
    CANONICAL = "canonical"  # PRD v2.3: Reused from canonical label


class QAPairRow(BaseModel):
    """A single Q&A pair row with rendered prompt and response (PRD v2.3: added canonical label support)."""

    row_index: int = Field(..., description="Index into the source sheet")

    # The actual rendered prompt (template + data merged)
    prompt: str = Field(..., description="Fully rendered prompt with data substituted")

    # Source data snapshot (for reference/debugging)
    source_data: dict[str, Any] = Field(
        default_factory=dict,
        description="Snapshot of source column values used to render the prompt",
    )

    # PRD v2.3: Reference to source item for canonical label lookup
    item_ref: str | None = Field(
        default=None,
        description="Reference to source item (e.g., inspection_id, invoice_id) for canonical label lookup",
    )

    # Response
    response: str | None = Field(
        default=None,
        description="The response value (AI-generated or human-labeled)",
    )
    response_source: ResponseSource = Field(
        default=ResponseSource.EMPTY,
        description="How the response was produced",
    )

    # PRD v2.3: Canonical label linkage
    canonical_label_id: str | None = Field(
        default=None,
        description="Link to canonical label (if response from canonical label)",
    )
    labeling_mode: LabelingMode = Field(
        default=LabelingMode.AI_GENERATED, description="How this Q&A pair was labeled"
    )

    # Metadata
    generated_at: datetime | None = None
    labeled_at: datetime | None = None
    labeled_by: str | None = None
    verified_at: datetime | None = None
    verified_by: str | None = None

    # Quality flags
    is_flagged: bool = Field(default=False, description="Flagged for review")
    flag_reason: str | None = None
    confidence_score: float | None = Field(
        default=None, ge=0, le=1, description="AI confidence in the response"
    )
    notes: str | None = Field(default=None, description="Additional notes from labeler")

    # PRD v2.3: Usage constraints (from canonical label or set directly)
    allowed_uses: list[str] | None = Field(
        default=None,
        description="Permitted usage types (training, validation, evaluation, few_shot, testing)",
    )
    prohibited_uses: list[str] | None = Field(
        default=None, description="Explicitly forbidden usage types"
    )


class TrainingSheet(BaseModel):
    """
    Materialized result of applying a template to a sheet.

    Contains actual prompt/response pairs (Q&A pairs) that can be used
    for labeling, review, and fine-tuning export.
    """

    id: str

    # Source references
    sheet_id: str = Field(..., description="ID of the source sheet")
    sheet_name: str | None = Field(
        default=None, description="Name of source sheet at generation time"
    )

    # Frozen template config (snapshot at generation time)
    template_config: TemplateConfig = Field(
        ..., description="Snapshot of template config used for this training sheet"
    )

    # PRD v2.3: Label type for canonical label matching
    template_label_type: str | None = Field(
        default=None,
        description="Label type from template (for canonical label lookup)",
    )

    # Training Sheet metadata
    status: TrainingSheetStatus = TrainingSheetStatus.ASSEMBLING
    total_rows: int = Field(default=0, description="Total number of Q&A pairs")

    # Statistics (PRD v2.3: added canonical_reused_count)
    ai_generated_count: int = Field(
        default=0, description="Rows with AI-generated responses"
    )
    human_labeled_count: int = Field(default=0, description="Rows with human labels")
    human_verified_count: int = Field(default=0, description="Rows verified by humans")
    canonical_reused_count: int = Field(
        default=0, description="PRD v2.3: Rows pre-approved via canonical labels"
    )
    flagged_count: int = Field(default=0, description="Rows flagged for review")
    empty_count: int = Field(default=0, description="Rows without responses")

    # Timestamps
    created_at: datetime | None = None
    created_by: str | None = None
    updated_at: datetime | None = None
    completed_at: datetime | None = None  # When generation finished

    # Error info (if status == FAILED)
    error_message: str | None = None

    class Config:
        from_attributes = True


# ============================================================================
# Request/Response Models
# ============================================================================


class CreateTrainingSheetRequest(BaseModel):
    """Request body for creating a training sheet from a sheet."""

    # Optional: override the sheet's attached template
    template_config: TemplateConfig | None = Field(
        default=None,
        description="Optional template override. If not provided, uses sheet's attached template.",
    )

    # Options
    row_limit: int | None = Field(
        default=None,
        ge=1,
        le=100000,
        description="Limit number of rows to generate (for preview/testing)",
    )
    generate_responses: bool = Field(
        default=False,
        description="If True, also run AI generation on Q&A pairs",
    )


class CreateTrainingSheetResponse(BaseModel):
    """Response from training sheet creation."""

    training_sheet_id: str
    sheet_id: str
    status: TrainingSheetStatus
    total_rows: int
    message: str | None = None


class TrainingSheetPreviewResponse(BaseModel):
    """Response for previewing Q&A pairs in a training sheet."""

    training_sheet_id: str
    rows: list[QAPairRow]
    total_rows: int
    preview_rows: int

    # Pagination info
    offset: int = Field(default=0, description="Current offset in result set")
    limit: int = Field(default=100, description="Limit used for this query")

    # Stats
    ai_generated_count: int
    human_labeled_count: int
    human_verified_count: int
    flagged_count: int
    empty_count: int = Field(default=0, description="Rows without responses")


class QAPairUpdate(BaseModel):
    """Request to update a single Q&A pair row."""

    response: str | None = Field(default=None, description="New response value")
    is_flagged: bool | None = Field(default=None, description="Flag for review")
    flag_reason: str | None = Field(default=None, description="Reason for flagging")
    mark_as_verified: bool = Field(default=False, description="Mark as human verified")


class GenerateRequest(BaseModel):
    """Request to generate AI responses for Q&A pairs."""

    row_indices: list[int] | None = Field(
        default=None,
        description="Specific rows to generate. If None, generates all empty rows.",
    )
    overwrite_existing: bool = Field(
        default=False,
        description="If True, regenerate even if response exists",
    )
    include_examples: bool = Field(
        default=True,
        description="Include human-labeled rows as few-shot examples",
    )


class GenerateResponse(BaseModel):
    """Response from AI generation on training sheet."""

    training_sheet_id: str
    generated_count: int
    failed_count: int
    errors: list[dict[str, Any]] | None = None


class ExportRequest(BaseModel):
    """Request to export training sheet for fine-tuning."""

    # Export destination
    volume_path: str = Field(
        ...,
        description="UC Volume path for JSONL output (e.g., /Volumes/catalog/schema/vol/data.jsonl)",
    )

    # Filter options
    include_sources: list[ResponseSource] | None = Field(
        default=None,
        description="Only include rows with these response sources. Default: human_labeled, human_verified",
    )
    exclude_flagged: bool = Field(
        default=True,
        description="Exclude flagged rows from export",
    )

    # Format options
    include_system_instruction: bool = Field(
        default=True,
        description="Include system instruction in training examples",
    )
    format: str = Field(
        default="openai_chat",
        description="Export format: openai_chat, anthropic, or gemini",
    )

    # Train/validation split
    train_split: float | None = Field(
        default=None,
        ge=0.5,
        le=0.95,
        description="Fraction for training set (0.5-0.95). If provided, creates separate train/val files.",
    )
    random_seed: int = Field(
        default=42,
        description="Random seed for reproducible train/val splits",
    )


class ExportResponse(BaseModel):
    """Response from export operation."""

    training_sheet_id: str
    volume_path: str
    examples_exported: int
    format: str
    excluded_count: int = Field(default=0, description="Rows excluded due to filters")

    # Train/val split info (when train_split is provided)
    train_path: str | None = Field(default=None, description="Path to training JSONL")
    val_path: str | None = Field(default=None, description="Path to validation JSONL")
    train_count: int | None = Field(
        default=None, description="Number of training examples"
    )
    val_count: int | None = Field(
        default=None, description="Number of validation examples"
    )


class TrainingSheetListResponse(BaseModel):
    """Response for listing training sheets."""

    training_sheets: list[TrainingSheet]
    total: int
    page: int
    page_size: int
